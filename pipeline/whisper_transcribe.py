import json
import os
import re
import time

import numpy as np
import requests
from openai import OpenAI
from pydub import AudioSegment

from config import OPENAI_API_KEY, ASSEMBLYAI_API_KEY, TRANSCRIBE_PROVIDER
from helpers.gpt_cleaner import clean_segments_with_gpt
from helpers.validators import assert_valid_whisper
from pipeline.constants import AUDIO_DIR, WHISPER_DIR

client = OpenAI(api_key=OPENAI_API_KEY)
ASSEMBLYAI_API_URL = "https://api.assemblyai.com/v2"


# ============================================================
# ‚≠ê 1. –ù–ê–î–Å–ñ–ù–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø ‚Äî –≠–ù–ï–†–ì–ï–¢–ò–ß–ï–°–ö–ò–ô VAD ‚≠ê
# ============================================================

def _detect_speech_regions(audio_path: str, frame_ms: int = 15):
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã —Ä–µ—á–∏ [start_ms, end_ms] –±–µ–∑ —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö VAD –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π.

    –ò—Å–ø–æ–ª—å–∑—É–µ–º —ç–Ω–µ—Ä–≥–∏—é –ø–æ–ª–æ—Å—ã 0..8–∫ –ì—Ü, —Å–∫–æ–ª—å–∑—è—â–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∏ –≥–∏—Å—Ç–µ—Ä–µ–∑–∏—Å, —á—Ç–æ–±—ã:
    - –Ω–µ –∑–∞–ª–∏–ø–∞—Ç—å –Ω–∞ —à—É–º–µ/–º—É–∑—ã–∫–µ,
    - –Ω–∞–¥—ë–∂–Ω–æ –Ω–∞–π—Ç–∏ —Ä–µ–∞–ª—å–Ω–æ–µ –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏ (–Ω–µ 0.0),
    - —É–≤–∞–∂–∞—Ç—å –ø–∞—É–∑—ã –º–µ–∂–¥—É —Ñ—Ä–∞–∑–∞–º–∏.
    """

    audio = AudioSegment.from_file(audio_path).set_channels(1).set_frame_rate(16000)
    samples = np.array(audio.get_array_of_samples()).astype(np.float32)

    if samples.size == 0:
        return []

    frame_len = int(16000 * frame_ms / 1000)
    if frame_len <= 0:
        frame_len = 240

    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º, —á—Ç–æ–±—ã –ø–æ—Ä–æ–≥–∏ –Ω–µ –∑–∞–≤–∏—Å–µ–ª–∏ –æ—Ç –≥—Ä–æ–º–∫–æ—Å—Ç–∏ —Ñ–∞–π–ª–∞
    peak = np.max(np.abs(samples)) or 1.0
    samples = samples / peak

    energies = []
    for i in range(0, len(samples), frame_len):
        frame = samples[i:i + frame_len]
        if frame.size == 0:
            break
        energies.append(float(np.mean(np.abs(frame))))

    if not energies:
        return []

    energies = np.array(energies)

    # –°–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ, —á—Ç–æ–±—ã –æ–¥–∏–Ω–æ—á–Ω—ã–µ –≤—ã–±—Ä–æ—Å—ã –Ω–µ –ø—Ä–µ–≤—Ä–∞—â–∞–ª–∏—Å—å –≤ ¬´—Ä–µ—á—å¬ª
    if len(energies) > 4:
        kernel = np.ones(5) / 5
        energies = np.convolve(energies, kernel, mode="same")

    noise_floor = np.percentile(energies, 20)
    speech_threshold = max(noise_floor * 3.5, np.percentile(energies, 70))
    release_threshold = speech_threshold * 0.55

    min_speech_frames = max(1, int(320 / frame_ms))  # >= 0.32s —Ä–µ—á–∏
    min_gap_frames = max(1, int(180 / frame_ms))      # >= 0.18s —Ç–∏—à–∏–Ω—ã, —á—Ç–æ–±—ã –∑–∞–∫—Ä—ã—Ç—å —Å–µ–≥–º–µ–Ω—Ç

    segments = []
    in_speech = False
    speech_start = 0
    below_count = 0

    for idx, energy in enumerate(energies):
        if not in_speech and energy >= speech_threshold:
            in_speech = True
            speech_start = idx
            below_count = 0
            continue

        if in_speech:
            if energy < release_threshold:
                below_count += 1
                if below_count >= min_gap_frames:
                    speech_end = idx - below_count + 1
                    if speech_end - speech_start >= min_speech_frames:
                        segments.append((speech_start, speech_end))
                    in_speech = False
                    below_count = 0
            else:
                below_count = 0

    if in_speech:
        speech_end = len(energies) - 1
        if speech_end - speech_start >= min_speech_frames:
            segments.append((speech_start, speech_end))

    if not segments:
        return []

    pad_ms = 120
    merged = []
    for start_f, end_f in segments:
        start_ms = max(0, start_f * frame_ms - pad_ms)
        end_ms = min(len(audio), (end_f + 1) * frame_ms + pad_ms)

        if merged and start_ms <= merged[-1][1] + 80:
            merged[-1][1] = max(merged[-1][1], end_ms)
        else:
            merged.append([start_ms, end_ms])

    return merged


def segment_by_silence(audio_path: str, full_text: str):
    """
    –î–µ–ª–∏–º –∞—É–¥–∏–æ –ø–æ –ø–∞—É–∑–∞–º (—ç–Ω–µ—Ä–≥–µ—Ç–∏—á–µ—Å–∫–∏–π VAD) ‚Üí —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º.
    """

    padded_segments = _detect_speech_regions(audio_path)

    if not padded_segments:
        # fallback: –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç
        audio = AudioSegment.from_file(audio_path)
        return [{
            "id": 0,
            "start": 0.0,
            "end": len(audio) / 1000,
            "text": full_text.strip()
        }]

    # ‚≠ê –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∞—Ä–∞–±—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    pattern = re.compile(r"[^.!?ÿü‚Ä¶]+(?:[.!?ÿü‚Ä¶]+|$)")
    sentences = [s.strip() for s in pattern.findall(full_text) if s.strip()]

    def count_words(s):
        return len(s.split())

    durations = [end - start for start, end in padded_segments]
    sentence_data = [(s, count_words(s)) for s in sentences]
    total_words = sum(cnt for _, cnt in sentence_data)

    whisper_segments = []

    for idx, (start, end) in enumerate(padded_segments):
        remaining_segments = len(padded_segments) - idx
        remaining_dur = sum(durations[idx:]) or 1

        if not sentence_data:
            break

        if remaining_segments == 1:
            picked = sentence_data
            sentence_data = []
        else:
            # –°–∫–æ–ª—å–∫–æ —Å–ª–æ–≤ —Ä–µ–∞–ª—å–Ω–æ –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –≤ —Å–µ–≥–º–µ–Ω—Ç (‚âà3.2 —Å–ª–æ–≤–∞/—Å–µ–∫)
            max_for_time = max(1, int(round((durations[idx] / 1000.0) * 3.2)))
            target_by_ratio = max(1, round(total_words * durations[idx] / remaining_dur))
            target = min(max_for_time, target_by_ratio)

            picked = []
            picked_words = 0

            while sentence_data:
                # –ß—Ç–æ–±—ã –≤–ø–µ—Ä–µ–¥–∏ –æ—Å—Ç–∞–ª—Å—è —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω sentence
                if len(sentence_data) <= (remaining_segments - 1) and picked:
                    break

                sent, count = sentence_data[0]

                # –ù–µ –ø–µ—Ä–µ–ø–æ–ª–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç
                if picked and picked_words + count > target:
                    break

                picked.append(sentence_data.pop(0))
                picked_words += count

            # safety fallback
            if not picked:
                picked.append(sentence_data.pop(0))

        text = " ".join(s for s, _ in picked).strip()
        total_words -= sum(cnt for _, cnt in picked)

        whisper_segments.append({
            "id": len(whisper_segments),
            "start": start / 1000,
            "end": end / 1000,
            "text": text
        })

    return whisper_segments


# ============================================================
# ‚≠ê 2. AssemblyAI –±–µ–∑ –∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ‚Äî –º—ã –¥–µ–ª–∞–µ–º —Å–≤–æ–∏ ‚≠ê
# ============================================================

def _transcribe_with_assemblyai(audio_path, expected_language=None):
    if not ASSEMBLYAI_API_KEY:
        raise RuntimeError("‚ùå ASSEMBLYAI_API_KEY missing")

    headers = {"authorization": ASSEMBLYAI_API_KEY}

    def _read_file(path):
        with open(path, "rb") as f:
            while chunk := f.read(5_242_880):
                yield chunk

    print("‚¨ÜÔ∏è  Uploading audio...")
    upload_resp = requests.post(
        f"{ASSEMBLYAI_API_URL}/upload",
        headers=headers,
        data=_read_file(audio_path)
    )
    upload_resp.raise_for_status()
    audio_url = upload_resp.json()["upload_url"]

    payload = {
        "audio_url": audio_url,
        "speech_model": "universal",
        "punctuate": True
    }
    if expected_language:
        payload["language_code"] = expected_language

    print("üõ∞  Requesting transcription...")
    resp = requests.post(
        f"{ASSEMBLYAI_API_URL}/transcript",
        json=payload,
        headers=headers
    )
    resp.raise_for_status()
    transcript_id = resp.json()["id"]

    poll_url = f"{ASSEMBLYAI_API_URL}/transcript/{transcript_id}"

    while True:
        poll = requests.get(poll_url, headers=headers).json()
        if poll["status"] == "completed":
            print("‚úÖ AssemblyAI transcription completed")
            break
        if poll["status"] == "error":
            raise RuntimeError(poll.get("error"))
        time.sleep(3)

    full_text = poll.get("text", "").strip()
    segments = segment_by_silence(audio_path, full_text)

    return {
        "text": full_text,
        "language": poll.get("language_code", expected_language),
        "segments": segments,
        "duration": poll.get("audio_duration")
    }


# ============================================================
# ‚≠ê 3. Whisper API ‚Üí —Å–µ–≥–º–µ–Ω—Ç—ã —Ç–æ–∂–µ –ø–æ –ø–∞—É–∑–∞–º ‚≠ê
# ============================================================

def _transcribe_with_whisper(audio_path, expected_language=None):
    with open(audio_path, "rb") as f:
        resp = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="verbose_json"
        )
    result = resp.model_dump()

    full_text = result.get("text", "")
    segments = segment_by_silence(audio_path, full_text)

    result["segments"] = segments
    return result


# ============================================================
# ‚≠ê 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚≠ê
# ============================================================

def whisper_transcribe(audio_file="input.wav", expected_language=None):
    audio_path = os.path.join(AUDIO_DIR, audio_file)

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"‚ùå Audio file not found ‚Üí {audio_path}")

    provider = TRANSCRIBE_PROVIDER.lower()
    print(f"üéß Transcribing using {provider}: {audio_path}")

    if provider == "whisper":
        data = _transcribe_with_whisper(audio_path, expected_language)
    else:
        data = _transcribe_with_assemblyai(audio_path, expected_language)

    os.makedirs(WHISPER_DIR, exist_ok=True)

    json_path = os.path.join(WHISPER_DIR, "transcript.json")
    txt_path = os.path.join(WHISPER_DIR, "transcript.txt")

    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(data, jf, ensure_ascii=False, indent=2)

    with open(txt_path, "w", encoding="utf-8") as tf:
        tf.write(data.get("text", ""))

    print("üìÑ JSON saved")
    print("üìù TXT saved")

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    assert_valid_whisper(json_path, expected_language)

    # GPT –æ—á–∏—Å—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    cleaned = clean_segments_with_gpt(data)

    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(cleaned, jf, ensure_ascii=False, indent=2)

    print("üü¢ Whisper validation PASSED")
    return json_path


if __name__ == "__main__":
    whisper_transcribe(expected_language="ar")