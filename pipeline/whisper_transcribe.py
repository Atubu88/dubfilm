import json
import os
import re
import time

import requests
from openai import OpenAI
from pydub import AudioSegment
from pydub.silence import split_on_silence

from config import OPENAI_API_KEY, ASSEMBLYAI_API_KEY, TRANSCRIBE_PROVIDER
from helpers.gpt_cleaner import clean_segments_with_gpt
from helpers.validators import assert_valid_whisper
from pipeline.constants import AUDIO_DIR, WHISPER_DIR

client = OpenAI(api_key=OPENAI_API_KEY)
ASSEMBLYAI_API_URL = "https://api.assemblyai.com/v2"


# ============================================================
# ‚≠ê 1. –ü–†–û–§–ï–°–°–ò–û–ù–ê–õ–¨–ù–ê–Ø –°–ï–ì–ú–ï–ù–¢–ê–¶–ò–Ø ‚Äî –ü–û –¢–ò–®–ò–ù–ï ‚≠ê
# ============================================================

def segment_by_silence(audio_path: str, full_text: str):
    """
    –î–µ–ª–∏–º –∞—É–¥–∏–æ –ø–æ –ø–∞—É–∑–∞–º ‚Üí –∑–∞—Ç–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –ø–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º.
    """

    audio = AudioSegment.from_file(audio_path)

    # üî• –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ —Ç–∏—à–∏–Ω—ã
    silence_threshold = audio.dBFS - 15

    chunks = split_on_silence(
        audio,
        min_silence_len=220,             # 0.22 —Å–µ–∫ ‚Üí –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∞—Ä–∞–±—Å–∫–æ–π —Ä–µ—á–∏
        silence_thresh=silence_threshold,
        keep_silence=80
    )

    if not chunks:
        # fallback: –æ–¥–∏–Ω —Å–µ–≥–º–µ–Ω—Ç
        return [{
            "id": 0,
            "start": 0.0,
            "end": len(audio) / 1000,
            "text": full_text.strip()
        }]

    # ‚≠ê –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∞—Ä–∞–±—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
    pattern = re.compile(r"[^.!?ÿü‚Ä¶]+(?:[.!?ÿü‚Ä¶]+|$)")
    sentences = [s.strip() for s in pattern.findall(full_text) if s.strip()]

    def count_words(s):
        return len(s.split())

    # –°—á–∏—Ç–∞–µ–º –¥–ª–∏–Ω—É –∫–∞–∂–¥–æ–≥–æ –∞—É–¥–∏–æ-—Å–µ–≥–º–µ–Ω—Ç–∞
    segments = []
    cursor = 0
    for chunk in chunks:
        start = cursor
        end = cursor + len(chunk)
        segments.append((start, end))
        cursor = end

    durations = [end - start for start, end in segments]
    total_duration = sum(durations) or 1

    # –ì–æ—Ç–æ–≤–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (sentence + word_count)
    sentence_data = [(s, count_words(s)) for s in sentences]
    total_words = sum(cnt for _, cnt in sentence_data)

    whisper_segments = []

    for idx, (start, end) in enumerate(segments):
        remaining_segments = len(segments) - idx

        if not sentence_data:
            break

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–µ–≥–º–µ–Ω—Ç ‚Äî –∫–ª–∞–¥—ë–º –≤—Å—ë –æ—Å—Ç–∞–≤—à–µ–µ—Å—è
        if remaining_segments == 1:
            picked = sentence_data
            sentence_data = []
        else:
            remaining_dur = sum(durations[idx:])
            target = max(1, round(total_words * durations[idx] / remaining_dur))

            picked = []
            picked_words = 0

            while sentence_data:
                # –ß—Ç–æ–±—ã –≤–ø–µ—Ä–µ–¥–∏ –æ—Å—Ç–∞–ª—Å—è —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω sentence
                if len(sentence_data) <= (remaining_segments - 1) and picked:
                    break

                sent, count = sentence_data[0]

                # –ù–µ –ø–µ—Ä–µ–ø–æ–ª–Ω—è–µ–º —Å–µ–≥–º–µ–Ω—Ç
                if picked and picked_words + count > target:
                    break

                picked.append(sentence_data.pop(0))
                picked_words += count

            # safety fallback
            if not picked:
                picked.append(sentence_data.pop(0))

        text = " ".join(s for s, _ in picked).strip()
        total_words -= sum(cnt for _, cnt in picked)

        whisper_segments.append({
            "id": len(whisper_segments),
            "start": start / 1000,
            "end": end / 1000,
            "text": text
        })

    return whisper_segments


# ============================================================
# ‚≠ê 2. AssemblyAI –±–µ–∑ –∏—Ö —Å–µ–≥–º–µ–Ω—Ç–æ–≤ ‚Äî –º—ã –¥–µ–ª–∞–µ–º —Å–≤–æ–∏ ‚≠ê
# ============================================================

def _transcribe_with_assemblyai(audio_path, expected_language=None):
    if not ASSEMBLYAI_API_KEY:
        raise RuntimeError("‚ùå ASSEMBLYAI_API_KEY missing")

    headers = {"authorization": ASSEMBLYAI_API_KEY}

    def _read_file(path):
        with open(path, "rb") as f:
            while chunk := f.read(5_242_880):
                yield chunk

    print("‚¨ÜÔ∏è  Uploading audio...")
    upload_resp = requests.post(
        f"{ASSEMBLYAI_API_URL}/upload",
        headers=headers,
        data=_read_file(audio_path)
    )
    upload_resp.raise_for_status()
    audio_url = upload_resp.json()["upload_url"]

    payload = {
        "audio_url": audio_url,
        "speech_model": "universal",
        "punctuate": True
    }
    if expected_language:
        payload["language_code"] = expected_language

    print("üõ∞  Requesting transcription...")
    resp = requests.post(
        f"{ASSEMBLYAI_API_URL}/transcript",
        json=payload,
        headers=headers
    )
    resp.raise_for_status()
    transcript_id = resp.json()["id"]

    poll_url = f"{ASSEMBLYAI_API_URL}/transcript/{transcript_id}"

    while True:
        poll = requests.get(poll_url, headers=headers).json()
        if poll["status"] == "completed":
            print("‚úÖ AssemblyAI transcription completed")
            break
        if poll["status"] == "error":
            raise RuntimeError(poll.get("error"))
        time.sleep(3)

    full_text = poll.get("text", "").strip()
    segments = segment_by_silence(audio_path, full_text)

    return {
        "text": full_text,
        "language": poll.get("language_code", expected_language),
        "segments": segments,
        "duration": poll.get("audio_duration")
    }


# ============================================================
# ‚≠ê 3. Whisper API ‚Üí —Å–µ–≥–º–µ–Ω—Ç—ã —Ç–æ–∂–µ –ø–æ –ø–∞—É–∑–∞–º ‚≠ê
# ============================================================

def _transcribe_with_whisper(audio_path, expected_language=None):
    with open(audio_path, "rb") as f:
        resp = client.audio.transcriptions.create(
            model="whisper-1",
            file=f,
            response_format="verbose_json"
        )
    result = resp.model_dump()

    full_text = result.get("text", "")
    segments = segment_by_silence(audio_path, full_text)

    result["segments"] = segments
    return result


# ============================================================
# ‚≠ê 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ‚≠ê
# ============================================================

def whisper_transcribe(audio_file="input.wav", expected_language=None):
    audio_path = os.path.join(AUDIO_DIR, audio_file)

    if not os.path.exists(audio_path):
        raise FileNotFoundError(f"‚ùå Audio file not found ‚Üí {audio_path}")

    provider = TRANSCRIBE_PROVIDER.lower()
    print(f"üéß Transcribing using {provider}: {audio_path}")

    if provider == "whisper":
        data = _transcribe_with_whisper(audio_path, expected_language)
    else:
        data = _transcribe_with_assemblyai(audio_path, expected_language)

    os.makedirs(WHISPER_DIR, exist_ok=True)

    json_path = os.path.join(WHISPER_DIR, "transcript.json")
    txt_path = os.path.join(WHISPER_DIR, "transcript.txt")

    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(data, jf, ensure_ascii=False, indent=2)

    with open(txt_path, "w", encoding="utf-8") as tf:
        tf.write(data.get("text", ""))

    print("üìÑ JSON saved")
    print("üìù TXT saved")

    # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    assert_valid_whisper(json_path, expected_language)

    # GPT –æ—á–∏—Å—Ç–∫–∞ —Å–µ–≥–º–µ–Ω—Ç–æ–≤
    cleaned = clean_segments_with_gpt(data)

    with open(json_path, "w", encoding="utf-8") as jf:
        json.dump(cleaned, jf, ensure_ascii=False, indent=2)

    print("üü¢ Whisper validation PASSED")
    return json_path


if __name__ == "__main__":
    whisper_transcribe(expected_language="ar")
